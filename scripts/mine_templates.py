#!/usr/bin/env python3
"""Mine log templates from structured HDFS logs using Drain."""

from __future__ import annotations

import argparse
import json
import logging
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Iterable, List

import pandas as pd

try:
    from drain3 import TemplateMiner
    from drain3.file_persistence import FilePersistence
    from drain3.template_miner_config import TemplateMinerConfig
except ImportError as exc:  # pragma: no cover - handled at runtime
    raise SystemExit(
        "drain3 is required to run this script. Install dependencies via `pip install -r scripts/requirements.txt`."
    ) from exc

LOGGER = logging.getLogger("mine_templates")


def _fetch_cluster(template_miner: TemplateMiner, cluster_id: int):
    clusters = getattr(template_miner.drain, "clusters", {})
    if isinstance(clusters, dict):
        return clusters.get(cluster_id)
    if hasattr(clusters, "__iter__"):
        for cluster in clusters:
            if getattr(cluster, "cluster_id", None) == cluster_id:
                return cluster
    return None


def parse_args(argv: Iterable[str] | None = None) -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Mine log templates from a structured CSV produced by the Go parser."
    )
    parser.add_argument(
        "--input-csv",
        default="data/HDFS.csv",
        help="Path to the CSV file generated by the Go parser (default: data/HDFS.csv)",
    )
    parser.add_argument(
        "--output-structured",
        default="preprocessed/HDFS.log_structured.csv",
        help="Destination for the CSV with assigned EventId/EventTemplate.",
    )
    parser.add_argument(
        "--templates-json",
        default="preprocessed/HDFS_templates.json",
        help="Path to write the serialized Drain state for reuse.",
    )
    parser.add_argument(
        "--event-map-json",
        default="preprocessed/event_id_map.json",
        help="Path to write the mapping of event IDs to templates.",
    )
    parser.add_argument(
        "--message-column",
        default="message",
        help="Column in the CSV that contains the log message text (default: message).",
    )
    parser.add_argument(
        "--config-file",
        default=None,
        help="Optional path to a Drain configuration file (INI format).",
    )
    parser.add_argument(
        "--parser-depth",
        type=int,
        default=4,
        help="Drain parser depth (default: 4 for HDFS).",
    )
    parser.add_argument(
        "--parser-max-children",
        type=int,
        default=100,
        help="Drain max children per node (default: 100).",
    )
    parser.add_argument(
        "--parser-sim-th",
        type=float,
        default=0.4,
        help="Drain similarity threshold (default: 0.4).",
    )
    parser.add_argument(
        "--parser-extra-delimiters",
        default="_",
        help="Extra delimiters for tokenization (default: '_').",
    )
    parser.add_argument(
        "--log-level",
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
        help="Logging verbosity level (default: INFO).",
    )
    return parser.parse_args(argv)


def setup_logging(level: str) -> None:
    logging.basicConfig(
        level=getattr(logging, level.upper()),
        format="%(asctime)s | %(levelname)s | %(message)s",
    )


def configure_template_miner(args: argparse.Namespace) -> TemplateMiner:
    config = TemplateMinerConfig()
    if args.config_file:
        config.load(args.config_file)
        LOGGER.info("Loaded Drain configuration from %s", args.config_file)

    if args.parser_depth is not None:
        config.drain_depth = args.parser_depth
    if args.parser_max_children is not None:
        config.drain_max_children = args.parser_max_children
    if args.parser_sim_th is not None:
        config.drain_sim_th = args.parser_sim_th
    if args.parser_extra_delimiters is not None:
        config.drain_extra_delimiters = args.parser_extra_delimiters

    config.profiling_enabled = False

    persistence_handler = FilePersistence(str(args.templates_json))
    return TemplateMiner(config=config, persistence_handler=persistence_handler)


def mine_templates(args: argparse.Namespace) -> None:
    input_path = Path(args.input_csv)
    if not input_path.exists():
        raise FileNotFoundError(f"Input CSV not found: {input_path}")

    output_structured_path = Path(args.output_structured)
    output_structured_path.parent.mkdir(parents=True, exist_ok=True)

    templates_json_path = Path(args.templates_json)
    templates_json_path.parent.mkdir(parents=True, exist_ok=True)

    event_map_json_path = Path(args.event_map_json)
    event_map_json_path.parent.mkdir(parents=True, exist_ok=True)

    LOGGER.info("Loading input CSV from %s", input_path)
    df = pd.read_csv(input_path)
    if args.message_column not in df.columns:
        raise KeyError(
            f"Column '{args.message_column}' not found in CSV. Available columns: {list(df.columns)}"
        )

    template_miner = configure_template_miner(args)

    cluster_to_event_id: Dict[int, str] = {}
    assigned_rows: List[Dict[str, object]] = []
    next_event_index = 1

    LOGGER.info("Processing %d log lines", len(df))
    for row in df.itertuples(index=False):
        row_dict = row._asdict()
        message = str(row_dict.get(args.message_column, ""))
        result = template_miner.add_log_message(message)
        cluster_id = result.get("cluster_id")
        if cluster_id is None:
            raise RuntimeError(
                f"Drain did not return a cluster_id for line index {len(assigned_rows)}"
            )

        cluster = _fetch_cluster(template_miner, cluster_id)
        template = (
            cluster.get_template() if cluster is not None else result.get("template_mined", "")
        )

        if cluster_id not in cluster_to_event_id:
            cluster_to_event_id[cluster_id] = f"E{next_event_index}"
            next_event_index += 1

        event_id = cluster_to_event_id[cluster_id]
        row_dict["EventId"] = event_id
        row_dict["EventTemplate"] = template
        assigned_rows.append(row_dict)

    structured_df = pd.DataFrame(assigned_rows)
    structured_df.to_csv(output_structured_path, index=False)
    LOGGER.info("Wrote structured log with templates to %s", output_structured_path)

    template_records = []
    for cluster_id, event_id in cluster_to_event_id.items():
        cluster = _fetch_cluster(template_miner, cluster_id)
        template_records.append(
            {
                "event_id": event_id,
                "cluster_id": cluster_id,
                "size": getattr(cluster, "size", None),
                "template": cluster.get_template() if cluster else None,
                "log_template_tokens": getattr(cluster, "log_template_tokens", None),
            }
        )

    template_records.sort(key=lambda item: int(item["event_id"][1:]))

    metadata = {
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "source_csv": str(input_path),
        "total_lines": len(df),
        "total_templates": len(template_records),
    }

    event_map_payload = {
        "metadata": metadata,
        "templates": template_records,
    }

    with event_map_json_path.open("w", encoding="utf-8") as f:
        json.dump(event_map_payload, f, ensure_ascii=False, indent=2)
    LOGGER.info("Wrote event ID map to %s", event_map_json_path)

    template_miner.save_state("manual_snapshot")
    LOGGER.info("Persisted Drain state to %s", templates_json_path)


def main(argv: Iterable[str] | None = None) -> None:
    args = parse_args(argv)
    setup_logging(args.log_level)

    try:
        mine_templates(args)
    except Exception as exc:  # pragma: no cover - top-level error reporting
        LOGGER.error("Template mining failed: %s", exc)
        raise


if __name__ == "__main__":
    main()
