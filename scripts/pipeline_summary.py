#!/usr/bin/env python3
"""Summarize pipeline outputs and validate predictions vs labels."""

from __future__ import annotations

import argparse
import csv
import json
from dataclasses import asdict, dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, Optional

import numpy as np
import pandas as pd

from guardrails import GuardrailError, GuardrailCheck, run_guardrails


@dataclass
class SequenceStats:
    total_sequences: int
    labeled_normals: Optional[int]
    labeled_anomalies: Optional[int]


@dataclass
class PredictionStats:
    total_predictions: int
    anomalies_flagged: int
    top_anomalies: list[Dict[str, Any]]


@dataclass
class EvaluationStats:
    true_positives: int
    false_positives: int
    true_negatives: int
    false_negatives: int
    precision: Optional[float]
    recall: Optional[float]
    f1: Optional[float]


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Summarize pipeline outputs and compute simple evaluation metrics."
    )
    parser.add_argument(
        "--sequences",
        type=Path,
        default=Path("preprocessed/HDFS_sequences.npz"),
        help="NPZ file produced by build_sequences.py (contains x_data/y_data).",
    )
    parser.add_argument(
        "--predictions",
        type=Path,
        default=Path("outputs/isolation_forest_predictions.csv"),
        help="CSV of predictions produced by isolation_forest_inference.py.",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("outputs/pipeline_run_summary.json"),
        help="Destination JSON file for the summary (directories created automatically).",
    )
    parser.add_argument(
        "--top-k",
        type=int,
        default=5,
        help="Include the top-k anomaly scores in the summary (default: 5).",
    )
    parser.add_argument(
        "--vocab-json",
        type=Path,
        default=Path("ml_models/isolation_forest_event_vocab.json"),
        help="Event vocabulary JSON produced during training (default: ml_models/isolation_forest_event_vocab.json).",
    )
    parser.add_argument(
        "--block-index",
        type=Path,
        default=Path("preprocessed/block_id_index.csv"),
        help="Block index CSV generated by build_sequences.py for length validation.",
    )
    parser.add_argument(
        "--event-drift-tolerance",
        type=float,
        default=0.25,
        help="Fractional tolerance for observed event count drift relative to the vocabulary size (default: 0.25).",
    )
    parser.add_argument(
        "--max-unknown-events",
        type=int,
        default=0,
        help="Maximum allowed number of events absent from the training vocabulary (default: 0).",
    )
    parser.add_argument(
        "--metrics-history",
        type=Path,
        default=Path("outputs/metrics_history.csv"),
        help="CSV file where per-run evaluation metrics will be appended (default: outputs/metrics_history.csv).",
    )
    return parser.parse_args()


def load_sequences(npz_path: Path) -> tuple[np.ndarray, Optional[np.ndarray]]:
    if not npz_path.exists():
        raise FileNotFoundError(f"Sequence artifact not found: {npz_path}")
    with np.load(npz_path, allow_pickle=True) as data:
        x_data = data["x_data"]
        y_data = data.get("y_data")
    return x_data, y_data


def compute_sequence_stats(x_data: np.ndarray, y_data: Optional[np.ndarray]) -> SequenceStats:
    total = int(len(x_data))
    if y_data is None:
        return SequenceStats(total_sequences=total, labeled_normals=None, labeled_anomalies=None)
    y_series = pd.Series(y_data)
    normals = int((y_series == 0).sum())
    anomalies = int((y_series == 1).sum())
    return SequenceStats(total_sequences=total, labeled_normals=normals, labeled_anomalies=anomalies)


def load_predictions(predictions_path: Path) -> tuple[pd.DataFrame, str]:
    if not predictions_path.exists():
        raise FileNotFoundError(f"Predictions CSV not found: {predictions_path}")
    df = pd.read_csv(predictions_path)
    required_columns = {"prediction", "anomaly_score"}
    missing = required_columns.difference(df.columns)
    if missing:
        raise ValueError(f"Predictions CSV is missing columns: {sorted(missing)}")
    if "score_direction" in df.columns:
        direction = df["score_direction"].iloc[0]
    else:
        direction = "lower"
    return df, str(direction)


def compute_prediction_stats(df: pd.DataFrame, top_k: int, score_direction: str) -> PredictionStats:
    anomalies_flagged = int((df["prediction"] == 1).sum())
    ascending = score_direction != "higher"
    top_rows = (
        df[df["prediction"] == 1]
        .sort_values("anomaly_score", ascending=ascending)
        .head(top_k)
    )
    top_payload = top_rows.to_dict(orient="records")
    return PredictionStats(
        total_predictions=int(len(df)),
        anomalies_flagged=anomalies_flagged,
        top_anomalies=top_payload,
    )


def compute_evaluation_stats(predictions: pd.DataFrame, labels: Optional[np.ndarray]) -> Optional[EvaluationStats]:
    if labels is None:
        return None
    if len(predictions) != len(labels):
        raise ValueError(
            "Prediction count does not match the number of labeled sequences. "
            f"predictions={len(predictions)}, labels={len(labels)}"
        )
    y_pred = predictions["prediction"].to_numpy()
    y_true = labels.astype(int)

    tp = int(((y_pred == 1) & (y_true == 1)).sum())
    fp = int(((y_pred == 1) & (y_true == 0)).sum())
    tn = int(((y_pred == 0) & (y_true == 0)).sum())
    fn = int(((y_pred == 0) & (y_true == 1)).sum())

    def safe_div(num: int, denom: int) -> Optional[float]:
        return round(num / denom, 4) if denom > 0 else None

    precision = safe_div(tp, tp + fp)
    recall = safe_div(tp, tp + fn)
    if precision is not None and recall is not None and (precision + recall) > 0:
        f1 = round(2 * precision * recall / (precision + recall), 4)
    else:
        f1 = None

    return EvaluationStats(
        true_positives=tp,
        false_positives=fp,
        true_negatives=tn,
        false_negatives=fn,
        precision=precision,
        recall=recall,
        f1=f1,
    )


def load_vocab(path: Path) -> dict[str, int]:
    if not path.exists():
        raise FileNotFoundError(f"Vocabulary JSON not found: {path}")
    with path.open("r", encoding="utf-8") as fp:
        payload = json.load(fp)
    return {str(event): int(index) for event, index in payload.items()}


def guardrail_summary(checks: list[GuardrailCheck]) -> dict[str, Any]:
    summary: dict[str, Any] = {}
    for check in checks:
        summary[check.name] = {
            "status": check.status,
            **check.details,
        }
    return summary


def main() -> None:
    args = parse_args()

    x_data, y_data = load_sequences(args.sequences)
    seq_stats = compute_sequence_stats(x_data, y_data)
    vocab_mapping = load_vocab(args.vocab_json)

    predictions_df, score_direction = load_predictions(args.predictions)
    pred_stats = compute_prediction_stats(
        predictions_df, top_k=args.top_k, score_direction=score_direction
    )
    eval_stats = compute_evaluation_stats(predictions_df, y_data)

    guardrail_failure: Optional[Exception] = None
    try:
        guardrail_checks = run_guardrails(
            sequences=x_data,
            labels=y_data,
            vocab_mapping=vocab_mapping,
            block_index_path=args.block_index,
            drift_tolerance=args.event_drift_tolerance,
            max_unknown_events=args.max_unknown_events,
        )
        guardrail_report = guardrail_summary(guardrail_checks)
    except GuardrailError as exc:
        guardrail_report = {
            "status": "fail",
            "error": str(exc),
        }
        # Still record the summary JSON for debugging, but exit with failure after writing.
        guardrail_failure = exc
    else:
        guardrail_failure = None

    summary = {
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "artifacts": {
            "sequences": str(args.sequences),
            "predictions": str(args.predictions),
            "vocabulary": str(args.vocab_json),
        },
        "sequence_stats": asdict(seq_stats),
        "prediction_stats": {
            **asdict(pred_stats),
            "score_direction": score_direction,
        },
        "evaluation": asdict(eval_stats) if eval_stats is not None else None,
        "guardrails": guardrail_report,
    }

    args.output.parent.mkdir(parents=True, exist_ok=True)
    with args.output.open("w", encoding="utf-8") as fp:
        json.dump(summary, fp, indent=2)

    print(json.dumps(summary, indent=2))

    if eval_stats is not None:
        metrics_row = {
            "generated_at": summary["generated_at"],
            "data_csv": str(args.sequences),
            "predictions_csv": str(args.predictions),
            "algorithm": predictions_df["algorithm"].iat[0] if "algorithm" in predictions_df.columns and len(predictions_df) else None,
            "precision": eval_stats.precision,
            "recall": eval_stats.recall,
            "f1": eval_stats.f1,
            "true_positives": eval_stats.true_positives,
            "false_positives": eval_stats.false_positives,
            "true_negatives": eval_stats.true_negatives,
            "false_negatives": eval_stats.false_negatives,
            "anomalies_flagged": pred_stats.anomalies_flagged,
            "total_sequences": seq_stats.total_sequences,
            "labeled_anomalies": seq_stats.labeled_anomalies,
            "labeled_normals": seq_stats.labeled_normals,
            "contamination": (
                round(seq_stats.labeled_anomalies / seq_stats.total_sequences, 6)
                if seq_stats.labeled_anomalies is not None and seq_stats.total_sequences
                else None
            ),
            "guardrails_status": "ok" if guardrail_failure is None else "fail",
        }
        args.metrics_history.parent.mkdir(parents=True, exist_ok=True)
        fieldnames = list(metrics_row.keys())
        file_exists = args.metrics_history.exists()
        with args.metrics_history.open("a", encoding="utf-8", newline="") as fp:
            writer = csv.DictWriter(fp, fieldnames=fieldnames)
            if not file_exists:
                writer.writeheader()
            writer.writerow(metrics_row)

    if guardrail_failure is not None:
        raise guardrail_failure


if __name__ == "__main__":
    main()
