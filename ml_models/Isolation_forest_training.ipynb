{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6890ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import time\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba72be6a",
   "metadata": {},
   "source": [
    "### Isolation Forest with Sequence Data\n",
    "This script trains an Isolation Forest model. It uses the event sequences from HDFS.npz and converts them into a numerical feature matrix (event counts) before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc1e7cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using project root: /Users/emmanuel/Documents/LAD\n",
      "Data path: /Users/emmanuel/Documents/LAD/data/HDFS.npz\n",
      "Model output path: /Users/emmanuel/Documents/LAD/ml_models/isolation_forest_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# File Paths\n",
    "def locate_project_root(marker_path: str = \"data/HDFS.npz\", max_depth: int = 5) -> Path:\n",
    "    current = Path.cwd()\n",
    "    for _ in range(max_depth + 1):\n",
    "        if (current / marker_path).exists():\n",
    "            return current\n",
    "        current = current.parent\n",
    "    raise FileNotFoundError(f\"Unable to locate '{marker_path}' within {max_depth} parent levels from {Path.cwd()}.\")\n",
    "\n",
    "PROJECT_ROOT = locate_project_root()\n",
    "DATA_PATH = PROJECT_ROOT / \"data\" / \"HDFS.npz\"\n",
    "MODEL_OUTPUT_PATH = PROJECT_ROOT / \"ml_models\" / \"isolation_forest_model.joblib\"\n",
    "\n",
    "print(f\"Using project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Model output path: {MODEL_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b40b8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Loading sequence data from HDFS.npz...\n",
      "\n",
      "--> Splitting raw sequence data into training and testing sets...\n",
      "Training sequences: 402542\n",
      "Test sequences: 172519\n",
      "\n",
      "--> Splitting raw sequence data into training and testing sets...\n",
      "Training sequences: 402542\n",
      "Test sequences: 172519\n"
     ]
    }
   ],
   "source": [
    "# --- Load Data ---\n",
    "print(\"--> Loading sequence data from HDFS.npz...\")\n",
    "with np.load(DATA_PATH, allow_pickle=True) as data:\n",
    "    X_sequences = data['x_data']\n",
    "    y = data['y_data']\n",
    "\n",
    "# --- Split Data into Training and Testing Sets (BEFORE Feature Engineering) ---\n",
    "print(\"\\n--> Splitting raw sequence data into training and testing sets...\")\n",
    "X_seq_train, X_seq_test, y_train, y_test = train_test_split(\n",
    "    X_sequences,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    " )\n",
    "\n",
    "# Convert labels to numpy arrays for downstream use\n",
    "y_train = np.array(y_train, dtype=int)\n",
    "y_test = np.array(y_test, dtype=int)\n",
    "\n",
    "print(f\"Training sequences: {len(X_seq_train)}\")\n",
    "print(f\"Test sequences: {len(X_seq_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff19c421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> Building event vocabulary from training data...\n",
      "Unique events in training data: 29\n",
      "No unseen events in the test data.\n",
      "\n",
      "--- Data Summary ---\n",
      "Training matrix shape: (402542, 29)\n",
      "Test matrix shape: (172519, 29)\n",
      "Number of anomalies in training set: 11787\n",
      "Number of anomalies in test set: 5051\n",
      "No unseen events in the test data.\n",
      "\n",
      "--- Data Summary ---\n",
      "Training matrix shape: (402542, 29)\n",
      "Test matrix shape: (172519, 29)\n",
      "Number of anomalies in training set: 11787\n",
      "Number of anomalies in test set: 5051\n"
     ]
    }
   ],
   "source": [
    "# --- Feature Engineering: Build Vocabulary on Training Data Only ---\n",
    "print(\"\\n--> Building event vocabulary from training data...\")\n",
    "\n",
    "def build_event_count_matrix(sequences, mapping):\n",
    "    matrix = lil_matrix((len(sequences), len(mapping)), dtype=np.int32)\n",
    "    for row_idx, seq in enumerate(sequences):\n",
    "        for event in seq:\n",
    "            idx = mapping.get(event)\n",
    "            if idx is not None:\n",
    "                matrix[row_idx, idx] += 1\n",
    "    return matrix.tocsr()\n",
    "\n",
    "train_events = [event for seq in X_seq_train for event in seq]\n",
    "unique_train_events = sorted(set(train_events))\n",
    "event_to_int = {event: idx for idx, event in enumerate(unique_train_events)}\n",
    "\n",
    "print(f\"Unique events in training data: {len(unique_train_events)}\")\n",
    "\n",
    "X_train = build_event_count_matrix(X_seq_train, event_to_int)\n",
    "X_test = build_event_count_matrix(X_seq_test, event_to_int)\n",
    "\n",
    "# Track unseen events in the test set (not present in training vocabulary)\n",
    "unseen_events = sorted({event for seq in X_seq_test for event in seq if event not in event_to_int})\n",
    "if unseen_events:\n",
    "    print(f\"Unseen events in test data (ignored in feature matrix): {len(unseen_events)}\")\n",
    "else:\n",
    "    print(\"No unseen events in the test data.\")\n",
    "\n",
    "print(\"\\n--- Data Summary ---\")\n",
    "print(f\"Training matrix shape: {X_train.shape}\")\n",
    "print(f\"Test matrix shape: {X_test.shape}\")\n",
    "print(f\"Number of anomalies in training set: {y_train.sum()}\")\n",
    "print(f\"Number of anomalies in test set: {y_test.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9371e990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--> Configuring model training and hyperparameter search...\n",
      "Calculated contamination on training set: 0.0293\n",
      "Contamination candidates for search: [0.029281, 0.043922, 0.05]\n",
      "--> Starting grid search (this may take several minutes)...\n",
      "\n",
      "--- Hyperparameter Search Summary ---\n",
      "Best parameters: {'contamination': 0.029281, 'max_features': 0.75, 'max_samples': 1.0, 'n_estimators': 100}\n",
      "Best cross-validated F1-score: 0.8005\n",
      "Model saved to '/Users/emmanuel/Documents/LAD/ml_models/isolation_forest_model.joblib'\n",
      "\n",
      "--> Evaluating on the test set...\n",
      "\n",
      "--- Hyperparameter Search Summary ---\n",
      "Best parameters: {'contamination': 0.029281, 'max_features': 0.75, 'max_samples': 1.0, 'n_estimators': 100}\n",
      "Best cross-validated F1-score: 0.8005\n",
      "Model saved to '/Users/emmanuel/Documents/LAD/ml_models/isolation_forest_model.joblib'\n",
      "\n",
      "--> Evaluating on the test set...\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.99      0.99      0.99    167468\n",
      "     Anomaly       0.79      0.78      0.78      5051\n",
      "\n",
      "    accuracy                           0.99    172519\n",
      "   macro avg       0.89      0.89      0.89    172519\n",
      "weighted avg       0.99      0.99      0.99    172519\n",
      "\n",
      "\n",
      "Total elapsed time: 4.71 minutes\n",
      "\n",
      "--- Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.99      0.99      0.99    167468\n",
      "     Anomaly       0.79      0.78      0.78      5051\n",
      "\n",
      "    accuracy                           0.99    172519\n",
      "   macro avg       0.89      0.89      0.89    172519\n",
      "weighted avg       0.99      0.99      0.99    172519\n",
      "\n",
      "\n",
      "Total elapsed time: 4.71 minutes\n"
     ]
    }
   ],
   "source": [
    "# --- Hyperparameter Tuning and Model Training ---\n",
    "print(\"\\n--> Configuring model training and hyperparameter search...\")\n",
    "\n",
    "contamination_train = y_train.sum() / len(y_train)\n",
    "print(f\"Calculated contamination on training set: {contamination_train:.4f}\")\n",
    "\n",
    "contamination_candidates = {round(float(contamination_train), 6)}\n",
    "if contamination_train < 0.05:\n",
    "    contamination_candidates.add(0.05)\n",
    "if contamination_train < 0.1:\n",
    "    contamination_candidates.add(round(float(min(0.1, contamination_train * 1.5)), 6))\n",
    "contamination_candidates = sorted(contamination_candidates)\n",
    "print(f\"Contamination candidates for search: {contamination_candidates}\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_samples': [0.8, 1.0],\n",
    "    'contamination': contamination_candidates,\n",
    "    'max_features': [0.75, 1.0]\n",
    "}\n",
    "\n",
    "def isolation_forest_f1(y_true, y_pred):\n",
    "    mapped = np.where(y_pred == -1, 1, 0)\n",
    "    return f1_score(y_true, mapped)\n",
    "\n",
    "scorer = make_scorer(isolation_forest_f1)\n",
    "iso_forest = IsolationForest(random_state=42, n_jobs=-1)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=iso_forest,\n",
    "    param_grid=param_grid,\n",
    "    scoring=scorer,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"--> Starting grid search (this may take several minutes)...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n--- Hyperparameter Search Summary ---\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validated F1-score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "model = grid_search.best_estimator_\n",
    "joblib.dump(model, MODEL_OUTPUT_PATH)\n",
    "print(f\"Model saved to '{MODEL_OUTPUT_PATH}'\")\n",
    "\n",
    "# --- Evaluation on Held-Out Test Set ---\n",
    "print(\"\\n--> Evaluating on the test set...\")\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_pred_mapped = np.where(y_test_pred == -1, 1, 0)\n",
    "\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "print(classification_report(y_test, y_test_pred_mapped, target_names=['Normal', 'Anomaly']))\n",
    "\n",
    "elapsed_minutes = (time.time() - start_time) / 60\n",
    "print(f\"\\nTotal elapsed time: {elapsed_minutes:.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
